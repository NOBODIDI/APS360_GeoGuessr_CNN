{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "satm_kJWXZfc",
        "5RzvBHbyQfn_"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "3b3868b196654e18a7363087a6650587": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_158af608a6bb4edd9728afefe204eae5",
              "IPY_MODEL_a71d19984c46402e8b1e4bfb3e08646e",
              "IPY_MODEL_f2755615c3da4e53950aba48684b87a5"
            ],
            "layout": "IPY_MODEL_a4e42f7c17ee4e27a24ef6feaff0cb7c"
          }
        },
        "158af608a6bb4edd9728afefe204eae5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8b3aeb38b3af493ca2812599e7e3fe01",
            "placeholder": "​",
            "style": "IPY_MODEL_052a5088603348848dff672617c7c664",
            "value": "100%"
          }
        },
        "a71d19984c46402e8b1e4bfb3e08646e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b36d4505a0fe42ef80ccbaf19441ced1",
            "max": 102540417,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1b03095b86f1430488d478bf064dd048",
            "value": 102540417
          }
        },
        "f2755615c3da4e53950aba48684b87a5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3f85c74bb3884918b75432871de22f7f",
            "placeholder": "​",
            "style": "IPY_MODEL_d1aee648ffc84d3eaca7633ca7360f1c",
            "value": " 97.8M/97.8M [00:02&lt;00:00, 49.4MB/s]"
          }
        },
        "a4e42f7c17ee4e27a24ef6feaff0cb7c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8b3aeb38b3af493ca2812599e7e3fe01": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "052a5088603348848dff672617c7c664": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b36d4505a0fe42ef80ccbaf19441ced1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1b03095b86f1430488d478bf064dd048": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3f85c74bb3884918b75432871de22f7f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d1aee648ffc84d3eaca7633ca7360f1c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NOBODIDI/APS360_GeoGuessr_NN/blob/main/testing/Distance_Computation_and_Visualizations.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Useful Links\n",
        "\n",
        "*   Background info: https://cs231n.github.io/transfer-learning/\n",
        "*   https://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html\n",
        "* Finetuning: https://pytorch.org/tutorials/beginner/finetuning_torchvision_models_tutorial.html\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Zqh8KJ5nURfR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wruYZ2HiCE8e",
        "outputId": "cc426763-245b-4717-9ac7-006bd01877ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "REMEMBER TO CHANGE RUNTIME TO GPU AND THEN MOUNT GOOGLE DRIVE (ALSO ADD TRAIN_VAL_TEST_SPLIT AS A SHORTCUT TO DRIVE)"
      ],
      "metadata": {
        "id": "G3qo_AEd0h1z"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GOL4hoqPs3dY"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.models as models\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import shutil\n",
        "import time\n",
        "import copy\n",
        "\n",
        "# For geographical distance computation\n",
        "import geopy\n",
        "from geopy.distance import great_circle \n",
        "\n",
        "# CSV\n",
        "import csv"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Initializing Model"
      ],
      "metadata": {
        "id": "Mg7MDPJW4cfs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Datasets and Dataloaders (Run ONCE)"
      ],
      "metadata": {
        "id": "JSI9t5Th_Z8j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading in Data\n",
        "\n",
        "# /////////// CHANGE THIS TO TRUE IF YOU WANT TO USE INCEPTION /////////\n",
        "is_inception = False\n",
        "# /////////////////////////////////////////////////////////\n",
        "if is_inception:\n",
        "    resize = 342\n",
        "    input_size = 299\n",
        "else:\n",
        "    resize = 256\n",
        "    input_size = 224\n",
        "\n",
        "# Data augmentation and normalization for training\n",
        "# Just normalization for validation, test\n",
        "\n",
        "data_transforms = {\n",
        "    'train': transforms.Compose([\n",
        "        transforms.RandomResizedCrop(input_size),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "    'val': transforms.Compose([\n",
        "        transforms.Resize(resize),\n",
        "        transforms.CenterCrop(input_size),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "     'test': transforms.Compose([\n",
        "        transforms.Resize(resize),\n",
        "        transforms.CenterCrop(input_size),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ])\n",
        "}\n",
        "\n",
        "# ////////////////// CHANGE THIS IF NEEDED TO POINT TO THE DATA ////////////////////////\n",
        "root_path = \"/content/drive/MyDrive/APS360 - Deep Learning Project /Colab Notebooks/32k_train_val_test\"\n",
        "# ////////////////////////////////////////////////////////////////////////////////\n",
        "\n",
        "# Fixed PyTorch random seed for reproducible result\n",
        "torch.manual_seed(1000)\n",
        "\n",
        "# Create training and validation and test datasets\n",
        "image_datasets = {x: torchvision.datasets.ImageFolder(os.path.join(root_path, x), \n",
        "                                                      data_transforms[x]) for x in ['train', 'val','test']}\n",
        "\n",
        "classes = image_datasets['train'].classes\n",
        "# Number of classes\n",
        "num_classes = len(classes)\n",
        "print(\"Total dataset:\",len(image_datasets['train'])+len(image_datasets['val'])+len(image_datasets['test']),\"\\n\",\n",
        "      \"\\tTrain size:\",len(image_datasets['train']),\"\\n\\tVal size:\",len(image_datasets['val']),\"\\n\\tTest size:\",len(image_datasets['test']))\n",
        "print(\"Number of classes:\",num_classes)\n",
        "\n",
        "batch_size = 64\n",
        "# Create training and validation and test dataloaders\n",
        "dataloaders_dict = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=batch_size, \n",
        "                                                   shuffle=True, num_workers=1) for x in ['train', 'val','test']}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M4JMTa2l6ARy",
        "outputId": "df1fd40a-ab87-4d68-cf63-b056838e312e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total dataset: 32000 \n",
            " \tTrain size: 22400 \n",
            "\tVal size: 4800 \n",
            "\tTest size: 4800\n",
            "Number of classes: 80\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import Trained Model Weights"
      ],
      "metadata": {
        "id": "RfZZTKna9fD5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Path to model with the hightest preliminary training accuracy\n",
        "model_path = \"/content/drive/MyDrive/APS360 - Deep Learning Project /Colab Notebooks/Hyperparameter Tuning/ResNet50 Results/Best model/best_32k_unfrozen_model_resnet50_bs64_lr0.001_ht10_wd1e-06_epochs40\"\n",
        "\n",
        "# Create model instance\n",
        "trained_model = models.resnet50(weights='DEFAULT')\n",
        "num_ftrs = trained_model.fc.in_features\n",
        "trained_model.fc = nn.Linear(num_ftrs, 80)\n",
        "\n",
        "# Load trained weights\n",
        "trained_model.load_state_dict(torch.load(model_path, map_location=torch.device('cpu')))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84,
          "referenced_widgets": [
            "3b3868b196654e18a7363087a6650587",
            "158af608a6bb4edd9728afefe204eae5",
            "a71d19984c46402e8b1e4bfb3e08646e",
            "f2755615c3da4e53950aba48684b87a5",
            "a4e42f7c17ee4e27a24ef6feaff0cb7c",
            "8b3aeb38b3af493ca2812599e7e3fe01",
            "052a5088603348848dff672617c7c664",
            "b36d4505a0fe42ef80ccbaf19441ced1",
            "1b03095b86f1430488d478bf064dd048",
            "3f85c74bb3884918b75432871de22f7f",
            "d1aee648ffc84d3eaca7633ca7360f1c"
          ]
        },
        "id": "F1dw6B549hEW",
        "outputId": "342e5264-c084-44f7-dd91-3cf2f31d692c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet50-11ad3fa6.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-11ad3fa6.pth\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0.00/97.8M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3b3868b196654e18a7363087a6650587"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Testing Model\n"
      ],
      "metadata": {
        "id": "satm_kJWXZfc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def test_model(model, \n",
        "               datasets=image_datasets, \n",
        "               batch_size=256 ):\n",
        "  \n",
        "    since = time.time()\n",
        "    dataloaders = {x: torch.utils.data.DataLoader(datasets[x], \n",
        "                                                  batch_size=batch_size, \n",
        "                                                  shuffle=True, \n",
        "                                                  num_workers=1) for x in ['test']}\n",
        "\n",
        "    model.eval()   # Set model to evaluate mode\n",
        "    running_corrects = 0\n",
        "\n",
        "    # Iterate over data.\n",
        "    for inputs, labels in dataloaders['test']:\n",
        "\n",
        "        #############################################\n",
        "        #To Enable GPU Usage\n",
        "        if use_cuda and torch.cuda.is_available():\n",
        "          model = model.cuda()\n",
        "          inputs = inputs.cuda()\n",
        "          labels = labels.cuda()\n",
        "        #############################################\n",
        "\n",
        "        # Get model outputs and calculate loss\n",
        "        outputs = model(inputs)\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "\n",
        "        # statistics\n",
        "        running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "    test_acc = float(running_corrects) / len(dataloaders['test'].dataset)\n",
        "\n",
        "    time_elapsed = time.time() - since\n",
        "    print('Testing complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
        "    print('Test Acc: {:4f}'.format(test_acc))\n",
        "\n",
        "    return"
      ],
      "metadata": {
        "id": "6YYaV4rbL-Ak"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Testing Model"
      ],
      "metadata": {
        "id": "5RzvBHbyQfn_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Run on GPU\n",
        "use_cuda = True\n",
        "if use_cuda and torch.cuda.is_available():\n",
        "  trained_model = trained_model.cuda()\n",
        "  print('CUDA is available!  Running on GPU ...')\n",
        "else:\n",
        "  print('CUDA is not available.  Running on CPU ...')\n",
        "\n",
        "test_model(trained_model, datasets=image_datasets, batch_size=64)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "THDFf2rbJDOC",
        "outputId": "68fa7fef-ad53-474a-ed32-ccb0ec99cfd0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA is not available.  Running on CPU ...\n",
            "Testing complete in 25m 35s\n",
            "Test Acc: 0.413125\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define Distance Evaluator Function\n",
        "This function returns the distance between two cordinates (latitude, longitude)"
      ],
      "metadata": {
        "id": "MrgtSuezbgYG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Reading the CSV and construct dictionary for geocell centre look-up\n",
        "################################### Replace hyperlink to csv file path #################################################\n",
        "csv_path = '/content/drive/MyDrive/APS360 - Deep Learning Project /Colab Notebooks/processed_geocells_final.csv'\n",
        "########################################################################################################################\n",
        "# Initialise empty dictionary\n",
        "class_centre_dict={}\n",
        "\n",
        "with open(csv_path,'r') as csv_file:\n",
        "  csv_reader = csv.reader(csv_file, delimiter=',') \n",
        "  counter=1\n",
        "  for row in csv_reader:\n",
        "    if counter!=1:\n",
        "      class_centre_dict[int(row[0])]=[float(row[5]),float(row[6])]\n",
        "    counter+=1\n",
        "\n",
        "# Put the lats and lons of each geocell centre into separate lists, ordered by\n",
        "# the order of the classes in the torch model outputs; convert the lists to tensors\n",
        "geocell_lats = [class_centre_dict[int(geocell_id)][0] for geocell_id in classes]\n",
        "geocell_lons = [class_centre_dict[int(geocell_id)][1] for geocell_id in classes]\n",
        "geocell_lats = torch.Tensor(geocell_lats).unsqueeze(0)\n",
        "geocell_lons = torch.Tensor(geocell_lons).unsqueeze(0)\n",
        "\n",
        "# Reading the filename to image coordinates dictionary\n",
        "################################### Replace hyperlink to csv file path #################################################\n",
        "csv_path = '/content/drive/MyDrive/APS360 - Deep Learning Project /Colab Notebooks/imagery_locations_merged.csv'\n",
        "########################################################################################################################\n",
        "# Initialise empty dictionary\n",
        "image_location_dict={}\n",
        "\n",
        "with open(csv_path,'r') as csv_file:\n",
        "  csv_reader = csv.reader(csv_file, delimiter=',') \n",
        "  counter=1\n",
        "  for row in csv_reader:\n",
        "    if counter!=1:\n",
        "      image_location_dict[row[1]]=[float(row[2]),float(row[3])]\n",
        "    counter+=1\n"
      ],
      "metadata": {
        "id": "WuO4MIZzcbme"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def model_distance(model, image_path, image_filename, true_class, print_results=True):\n",
        "    #def model_distance(model, root_path, dataset, true_class, image_filename):\n",
        "    '''\n",
        "    This function computes the distance of between the center of the model's prediction class and the image's true location given its filename and specified folders\n",
        "\n",
        "    Dataset sor can be either 'test', 'val', or 'train'\n",
        "    '''\n",
        "    model.eval()\n",
        "\n",
        "    ###################### GENERATE PATH #############################\n",
        "    #image_path = os.path.join(root_path, dataset, true_class, image_filename)\n",
        "    image = plt.imread(image_path,0)\n",
        "    # Repeat dataset transforms\n",
        "    transform = transforms.Compose([transforms.ToTensor(), \n",
        "                                transforms.Resize(256), \n",
        "                                transforms.CenterCrop(224), \n",
        "                                transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])\n",
        "    # Apply transforms\n",
        "    image_torch = transform(image)\n",
        "    # # Converts tensor dimensions C,W,H -> W,H,C\n",
        "    # image_torch.permute(1, 2, 0)\n",
        "    # plt.imshow(image_torch)\n",
        "    # Convert 3D tensor -> 4D tensor\n",
        "    image_torch = image_torch.unsqueeze(0)\n",
        "\n",
        "    # Get raw model outputs\n",
        "    outputs = model(image_torch)\n",
        "    outputs = F.softmax(outputs, dim=1) # probs now all sum to 1 and in range of [0,1]\n",
        "\n",
        "    if print_results:\n",
        "        print(f'True geocell of image: {true_class}')\n",
        "\n",
        "    # Basic method: taking centre of output class with highest probability\n",
        "    predicted_class = classes[outputs.argmax(dim=1)]\n",
        "    predicted_prob = torch.max(outputs, dim=1).values.item()\n",
        "    centre_of_prediction_class = class_centre_dict.get(int(predicted_class))\n",
        "    if print_results:\n",
        "        print(f'Predicted geocell with highest probability: {predicted_class} with confidence {predicted_prob:.4f}')\n",
        "        # print(f'Predicted image location as centre of most likely geocell: {centre_of_prediction_class}')\n",
        "\n",
        "    # More complex method: taking the weighted average of centres of all classes,\n",
        "    # based on predicted probabilities \n",
        "    # Take weighted sum of all lats (element-wise mult between probs and lats)\n",
        "    predicted_avg_lat = torch.mul(outputs, geocell_lats).sum(dim=1)\n",
        "    predicted_avg_lon = torch.mul(outputs, geocell_lons).sum(dim=1) # repeat for lons\n",
        "    predicted_avg_location = [predicted_avg_lat[0].item(), predicted_avg_lon[0].item()]\n",
        "    if print_results:\n",
        "        # print(f'Predicted image location as weighted average of all geocell centres: {predicted_avg_location}')\n",
        "        print(f'Predicted image location: {predicted_avg_location}')\n",
        "\n",
        "    # Retrieve image location\n",
        "    image_location = image_location_dict.get(image_filename)\n",
        "    if print_results:\n",
        "        print(f'True location of image: {image_location}')\n",
        "\n",
        "    # Calculate distance errors\n",
        "    distance_centre = great_circle(centre_of_prediction_class, image_location).km\n",
        "    distance_weighted = great_circle(predicted_avg_location, image_location).km\n",
        "\n",
        "    if print_results:\n",
        "        # print(f'Distance error to centre of most likely geocell: {distance_centre:.2f} km')\n",
        "        # print(f'Distance error to weighted average of all geocell centres: {distance_weighted:.2f} km')\n",
        "        print(f'Distance error: {distance_weighted:.2f} km')\n",
        "\n",
        "    map_vis = create_output_visualization(outputs, true_class, \n",
        "                                          centre_of_prediction_class, \n",
        "                                          predicted_avg_location, image_location)\n",
        "\n",
        "    return map_vis, int(predicted_class), distance_centre, distance_weighted, predicted_avg_location\n"
      ],
      "metadata": {
        "id": "uy3YzFKJ54y5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import folium\n",
        "import pandas as pd\n",
        "from IPython.display import display\n",
        "\n",
        "geocells = pd.read_csv('/content/drive/MyDrive/APS360 - Deep Learning Project /Colab Notebooks/processed_geocells_final.csv')\n",
        "\n",
        "def create_output_visualization(outputs, true_class, predicted_centre, predicted_weighted, true_loc):\n",
        "\n",
        "    m = folium.Map(location=[44.967243, -103.771556], zoom_start=3, tiles='cartodb positron')\n",
        "\n",
        "    for i, geocell_prob in enumerate(outputs[0]):\n",
        "        prob_class = int(classes[i])\n",
        "        geocell_info = geocells[geocells['geocell_id'] == prob_class].squeeze()\n",
        "\n",
        "        # draw geocell rectangle\n",
        "        folium.vector_layers.Rectangle(\n",
        "            bounds=[\n",
        "                (geocell_info['lat_top'], geocell_info['lon_left']),\n",
        "                (geocell_info['lat_top'], geocell_info['lon_right']),\n",
        "                (geocell_info['lat_bottom'], geocell_info['lon_right']),\n",
        "                (geocell_info['lat_bottom'], geocell_info['lon_left'])\n",
        "            ], \n",
        "            color='gray', fill=True, fill_color='green', \n",
        "            fill_opacity=geocell_prob.item() * 0.7, \n",
        "            tooltip=f'ID: {prob_class}; prob: {geocell_prob * 100:.2f}%').add_to(m)\n",
        "        \n",
        "    # add markers for true location and the two guesses\n",
        "    sv_url = f'<a href=http://maps.google.com/maps?q=&layer=c&cbll={true_loc[0]},{true_loc[1]} target=_blank>' \\\n",
        "             f'{true_loc[0]},{true_loc[1]}</a>'\n",
        "    folium.Marker(location=true_loc, \n",
        "                  icon=folium.Icon(color='black', icon='flag'), \n",
        "                  tooltip='True Location', popup=sv_url).add_to(m)\n",
        "    # folium.Marker(location=predicted_centre, \n",
        "    #               icon=folium.Icon(color='red'), \n",
        "    #               tooltip='Predicted Location (Centre)').add_to(m)\n",
        "    folium.Marker(location=predicted_weighted, \n",
        "                  icon=folium.Icon(color='orange'), \n",
        "                  tooltip='Predicted Location (Weighted)').add_to(m)\n",
        "\n",
        "    # add lines from guesses to true location\n",
        "    # folium.vector_layers.PolyLine(locations=[true_loc, predicted_centre], color='red').add_to(m)\n",
        "    folium.vector_layers.PolyLine(locations=[true_loc, predicted_weighted], color='orange').add_to(m)\n",
        "\n",
        "    return m"
      ],
      "metadata": {
        "id": "Y_mIm4EMeqA4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ----- Tests (Don't need to run this for demo) ------\n",
        "\n",
        "root_path = '/content/drive/MyDrive/APS360 - Deep Learning Project /Colab Notebooks/32k_train_val_test/test'\n",
        "\n",
        "# define tests as a list of tuples of (true class, filename without extension) \n",
        "tests = [\n",
        "    # (100, '-0VT1h4kR6JMngUUTkFCrQ'),\n",
        "    # (41, 'bZEnG1EgWjUfsvzrJpvQTQ'),\n",
        "    # (206, 'Q5IA8WBh9YDoDhA8GWuogQ'),\n",
        "    # (66, 'Drojr30vColtzLzVGSzHbg'),\n",
        "    # (93, 'lqxGPPDv1S49MBF-fbA6Yw'),\n",
        "    # (114, 'K5gYX0MYXlcwiqBYTZKXVQ'),\n",
        "    (22, '-Gr7SS91nEotBWPs7OSqGw'),\n",
        "    # (22, 'rlCnuV5ZRD1wEUUL6TBIbg')\n",
        "]\n",
        "\n",
        "for i, test in enumerate(tests):\n",
        "    print(f'----- Test {i+1} -----')\n",
        "    image_path = root_path + f'/{test[0]}/{test[1]}.png'\n",
        "    map_vis, _, _, _, _ = model_distance(trained_model, image_path, test[1], test[0])\n",
        "    print()\n",
        "    display(map_vis)\n"
      ],
      "metadata": {
        "id": "nuzBj4FHZk9Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Demo"
      ],
      "metadata": {
        "id": "-FQLlS_2FGbq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Look Up Image IDs from Test Lat/Lons"
      ],
      "metadata": {
        "id": "R7cvmWOgFNvz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "img_metadata_db = pd.read_csv('/content/drive/MyDrive/APS360 - Deep Learning Project /Colab Notebooks/final_representative_tests/imagery_locations_32k_test.csv')\n",
        "root_path = '/content/drive/MyDrive/APS360 - Deep Learning Project /Colab Notebooks/32k_train_val_test/test'\n",
        "# img_metadata_db = pd.read_csv('/content/drive/MyDrive/APS360 - Deep Learning Project /Colab Notebooks/final_representative_tests/imagery_locations_test_new.csv')\n",
        "# root_path = '/content/drive/MyDrive/APS360 - Deep Learning Project /Colab Notebooks/dataset4_28000'\n",
        "\n",
        "use_random = False\n",
        "\n",
        "if not use_random:\n",
        "\n",
        "    # ///////////////////////////////////////////////////////////////\n",
        "    # Enter lat/lon of test locations from Google Maps / GeoGuessr \n",
        "    # ///////////////////////////////////////////////////////////////\n",
        "    test_latlons = [\n",
        "        (34.575479,-100.954848), # Location 0\n",
        "        (46.22356,-105.685128), # Location 1\n",
        "        (17.10401,-96.847175), # Location 2\n",
        "        (45.941962,-87.207782), # Location 3\n",
        "        (34.768993,-81.017117), # Location 4\n",
        "        (36.096242,-111.357211), # 5\n",
        "        (47.001163,-54.136505), # 6\n",
        "        (51.270838,-107.78829), # 7\n",
        "        (16.690046,-92.017375), # 8\n",
        "        (46.246886,-94.973527), # 9\n",
        "    ]\n",
        "\n",
        "    test_imgs = []\n",
        "\n",
        "    for lat, lon in test_latlons:\n",
        "        # get closest match in terms of coordinates\n",
        "        img_metadata_db['coords_diff'] = (img_metadata_db['img_lat'] - lat).abs() + \\\n",
        "                                        (img_metadata_db['img_lon'] - lon).abs()\n",
        "        img_row = img_metadata_db[img_metadata_db['coords_diff'] == img_metadata_db['coords_diff'].min()].squeeze()\n",
        "        test_imgs.append((img_row['geocell_id'], img_row['pano_id']))\n",
        "\n",
        "else:\n",
        "\n",
        "    # OR use random test images\n",
        "    # random_df = img_metadata_db.sample(50, ignore_index=True) # randomly sample some images from set\n",
        "    random_df = img_metadata_db.copy() # use all images\n",
        "    test_imgs = list(zip(random_df['geocell_id'].tolist(), random_df['pano_id'].tolist()))\n"
      ],
      "metadata": {
        "id": "qOTr20V-Ffws"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To save any generated random set:"
      ],
      "metadata": {
        "id": "NYuDLFQSvtFo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "random_df.to_csv('/content/drive/MyDrive/APS360 - Deep Learning Project /Colab Notebooks/final_representative_tests/rand50_2.csv', index=False)"
      ],
      "metadata": {
        "id": "JdJ77puqvJBX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Get Model Predictions and Results\n"
      ],
      "metadata": {
        "id": "Z25ejJbzFiBs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Small dataset / individual map visualizations:"
      ],
      "metadata": {
        "id": "kxFVW8s1XQ_Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# PARAMETERS\n",
        "run_aggregate = False\n",
        "\n",
        "# ------------------------- Get Aggregate Results ------------------------------\n",
        "if run_aggregate:\n",
        "\n",
        "    import statistics as st\n",
        "\n",
        "    correct_class_count = 0\n",
        "    distance_errors_centre = []\n",
        "    distance_errors_weighted = []\n",
        "\n",
        "    for img_class, img_id in test_imgs:\n",
        "        image_path = root_path + f'/{img_class}/{img_id}.png'\n",
        "        _, predicted_class, distance_centre, distance_avg, _ = model_distance(trained_model, image_path, img_id, img_class, print_results=False)\n",
        "\n",
        "        correct_class_count += int(img_class == predicted_class)\n",
        "        distance_errors_centre.append(distance_centre)\n",
        "        distance_errors_weighted.append(distance_avg)\n",
        "\n",
        "    print('----- Aggregate Statistics -----')\n",
        "    print(f'Number of locations: {len(test_imgs)}')\n",
        "    print(f'Correctly predicted classes: {correct_class_count} out of {len(test_imgs)}')\n",
        "    # print(f'Total distance error (centre): {sum(distance_errors_centre):.2f} km')\n",
        "    print(f'Total distance error: {sum(distance_errors_weighted):.2f} km')\n",
        "    # print(f'Average distance error per location (centre): {st.mean(distance_errors_centre):.2f} km')\n",
        "    print(f'Average distance error per location: {st.mean(distance_errors_weighted):.2f} km')\n",
        "    # print(f'Max distance error per location (centre): {max(distance_errors_centre):.2f} km')\n",
        "    print(f'Max distance error per location: {max(distance_errors_weighted):.2f} km (location {np.argmax(distance_errors_weighted)})')\n",
        "    # print(f'Min distance error per location (centre): {min(distance_errors_centre):.2f} km')\n",
        "    print(f'Min distance error per location: {min(distance_errors_weighted):.2f} km (location {np.argmin(distance_errors_weighted)})')\n",
        "    # print(f'Median distance error per location (centre): {st.median(distance_errors_centre):.2f} km')\n",
        "    print(f'Median distance error per location: {st.median(distance_errors_weighted):.2f} km')\n",
        "    print()\n",
        "\n",
        "# ------------------------- View Results One-by-One ----------------------------\n",
        "if not run_aggregate:\n",
        "\n",
        "    cur_test_index = 4 # ////// Change this to view results of each test image in the defined set\n",
        "    cur_test_img_class = test_imgs[cur_test_index][0]\n",
        "    cur_test_img_id = test_imgs[cur_test_index][1]\n",
        "\n",
        "    print(f'----- Results for Location {cur_test_index} -----')\n",
        "    print(f'Image ID: {cur_test_img_id}')\n",
        "    image_path = root_path + f'/{cur_test_img_class}/{cur_test_img_id}.png'\n",
        "    map_vis, _, _, _, _ = model_distance(trained_model, image_path, cur_test_img_id, cur_test_img_class)\n",
        "    print()\n",
        "    display(map_vis)"
      ],
      "metadata": {
        "id": "MbgotB1AFq5-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Large dataset - results stored to file"
      ],
      "metadata": {
        "id": "0ytejVh5XXj_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "demo_output = random_df.copy()\n",
        "demo_output['predicted_geocell'] = 0 # initialize column in order to use int datatype\n",
        "\n",
        "for i in tqdm(range(len(demo_output))):\n",
        "    img_row = demo_output.iloc[i].squeeze()\n",
        "    img_class = img_row['geocell_id']\n",
        "    img_id = img_row['pano_id']\n",
        "\n",
        "    image_path = root_path + f'/{img_class}/{img_id}.png'\n",
        "    _, predicted_class, _, distance_avg, pred_loc = model_distance(trained_model, image_path, img_id, img_class, print_results=False)\n",
        "    demo_output.loc[i, 'predicted_geocell'] = predicted_class\n",
        "    demo_output.loc[i, 'predicted_lat'] = pred_loc[0]\n",
        "    demo_output.loc[i, 'predicted_lon'] = pred_loc[1]\n",
        "    demo_output.loc[i, 'distance_error'] = distance_avg\n",
        "\n",
        "demo_output['distance_error'] = demo_output['distance_error'].round(2)\n",
        "\n",
        "demo_output.to_csv('/content/drive/MyDrive/APS360 - Deep Learning Project /Colab Notebooks/final_representative_tests/new_test_all_results.csv', index=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qqH6OChpXag-",
        "outputId": "ca8f54ff-c836-4685-ec67-66b2f41d3812"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4800/4800 [43:43<00:00,  1.83it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Aggregate statistics and visualizations:"
      ],
      "metadata": {
        "id": "hs98GR-wYIL0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from IPython.display import display\n",
        "\n",
        "demo_output = pd.read_csv('/content/drive/MyDrive/APS360 - Deep Learning Project /Colab Notebooks/final_representative_tests/32k_test_all_results.csv')"
      ],
      "metadata": {
        "id": "pKhS-aMCYPZ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Histogram of distance errors:"
      ],
      "metadata": {
        "id": "-3WZieP4nZe2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'Total number of locations tested: {len(demo_output)}')\n",
        "print(f\"Average: {demo_output['distance_error'].mean():.2f} km\")\n",
        "print(f\"Minimum: {demo_output['distance_error'].min()} km\")\n",
        "print(f\"Maximum: {demo_output['distance_error'].max()} km\")\n",
        "print(f\"Median: {demo_output['distance_error'].median()} km\")\n",
        "\n",
        "BIN_WIDTH = 100\n",
        "ceiling = (int(demo_output['distance_error'].max() / BIN_WIDTH) + 1) * BIN_WIDTH # round up max value to the nearest multiple of BIN_WIDTH\n",
        "\n",
        "plt.figure(figsize=(8, 4), dpi=72)\n",
        "ax = plt.gca()\n",
        "_, bins, _ = plt.hist(demo_output['distance_error'], bins=int(ceiling / BIN_WIDTH), range=(0, ceiling), rwidth=0.9)\n",
        "# bins\n",
        "plt.xlabel('Distance Error of Prediction (km)')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title('Distribution of Location Prediction Distance Errors for Test Dataset')\n",
        "plt.grid(True)\n",
        "ax.minorticks_on()\n",
        "\n",
        "# plt.savefig('/content/drive/MyDrive/APS360 - Deep Learning Project /Colab Notebooks/final_representative_tests/32k_test_histogram.png')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 382
        },
        "id": "mA-NUywOYVKk",
        "outputId": "2a8da208-ec5e-434d-ddb8-d9da85865541"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of locations tested: 4800\n",
            "Average: 748.44 km\n",
            "Minimum: 1.18 km\n",
            "Maximum: 5326.23 km\n",
            "Median: 485.93 km\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 576x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Map visualization of geocell relative prediction error frequency:"
      ],
      "metadata": {
        "id": "un2lvKbZncpZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import branca\n",
        "\n",
        "demo_output['is_correct'] = (demo_output['geocell_id'] == demo_output['predicted_geocell'])\n",
        "print(demo_output['is_correct'].sum())\n",
        "wrong_pred = demo_output.groupby('geocell_id')['is_correct'].apply(lambda x: 1 - (x.sum() / len(x)))\n",
        "wrong_pred = wrong_pred.sort_values(ascending=False)\n",
        "wrong_pred = (wrong_pred - wrong_pred.min()) / (wrong_pred.max() - wrong_pred.min()) # normalize to between 0 and 1\n",
        "wrong_pred = wrong_pred.round(4)\n",
        "# wrong_pred\n",
        "\n",
        "geocells = pd.read_csv('/content/drive/MyDrive/APS360 - Deep Learning Project /Colab Notebooks/processed_geocells_final.csv')\n",
        "\n",
        "m = folium.Map(location=[44.967243, -103.771556], zoom_start=3, tiles='cartodb positron')\n",
        "\n",
        "colormap = branca.colormap.linear.YlOrRd_09.scale(wrong_pred.min(), wrong_pred.max(), max_labels=5)\n",
        "colormap.caption = 'Normalized Relative Class Prediction Error Rate'\n",
        "\n",
        "for i, geocell_info in geocells.iterrows():\n",
        "    # draw geocell rectangle\n",
        "    folium.vector_layers.Rectangle(\n",
        "        bounds=[\n",
        "            (geocell_info['lat_top'], geocell_info['lon_left']),\n",
        "            (geocell_info['lat_top'], geocell_info['lon_right']),\n",
        "            (geocell_info['lat_bottom'], geocell_info['lon_right']),\n",
        "            (geocell_info['lat_bottom'], geocell_info['lon_left'])\n",
        "        ], \n",
        "        color='gray', fill=True, fill_color=colormap(wrong_pred[int(geocell_info['geocell_id'])]), \n",
        "        fill_opacity=0.6, \n",
        "        tooltip=f\"ID: {int(geocell_info['geocell_id'])}; normalized error rate: {wrong_pred[geocell_info['geocell_id']]}\").add_to(m)\n",
        "\n",
        "colormap.add_to(m)\n",
        "\n",
        "m"
      ],
      "metadata": {
        "id": "4MuQ3xUxZrgG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}